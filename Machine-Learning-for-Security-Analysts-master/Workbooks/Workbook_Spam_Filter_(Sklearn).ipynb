{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Workbook: Spam Filter (Sklearn).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "SwawBDPuhgUP",
        "dPtTuWPDhy_Y",
        "e4BvaXgBOmxv",
        "2h_fentliZuA",
        "js7XaCCEPJtZ",
        "iAh5AQOwP72_",
        "q4PmEWehhCDo",
        "Oh5CNmcGQ_lk",
        "UujcLc_sUSdI",
        "YLMzUWT5VQMk",
        "TeE--QtjjhIV",
        "F8XRvFQiWYDB",
        "L08z7zxuYNs3",
        "UyiNCVFlloOL",
        "cndMVm3Qht9r",
        "F7hyvnwxj5y-",
        "3KmNRsISkTnc",
        "VgFq9cZ4kbzP",
        "C0aUsH92lJzq"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SwawBDPuhgUP",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning for Security Analysts - Workbook </br> Spam Filter (Scikit-learn)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "Author: GTKlondike\n",
        "</br>\n",
        "Email: GTKlondike@gmail.com\n",
        "</br>\n",
        "YouTube: [NetSec Explained](https://www.youtube.com/channel/UCsKK7UIiYqvK35aWrCCgUUA)\n",
        "\n",
        "**Dataset:** https://github.com/NetsecExplained/Machine-Learning-for-Security-Analysts\n",
        "\n",
        "**Goal:** This workbook will walk you through the steps to build, train, test, and evaluate a series of spam classifiers using the Scikit-learn machine learning library\n",
        "\n",
        "**Outline:** \n",
        "* Initial Setup\n",
        "* Tokenization\n",
        "* Load Training Data\n",
        "* Vectorize Training Data\n",
        "* Load Testing Data\n",
        "* Train and Evaluate Models\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxS4n9Gfyvw2",
        "colab_type": "text"
      },
      "source": [
        "## Instructions\n",
        "To use Jupyter notebooks:\n",
        "* To run a cell, click on the play button to the left of the code or pressh shift+enter\n",
        "* You will see a busy indicator in the top left area while the runtime is executing\n",
        "* A number will appear when the cell is done\n",
        "\n",
        "To complete the workbook:\n",
        "* Step through the workbook, completing the tasks in order"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjMomkNIzsJd",
        "colab_type": "text"
      },
      "source": [
        "# Initial Setup\n",
        "We'll start by downloading the data and loading the needed libraries."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YM57dpKHMi8z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Download data from Github\n",
        "! git clone https://github.com/NetsecExplained/Machine-Learning-for-Security-Analysts.git\n",
        "  \n",
        "# Install dependencies\n",
        "! pip install nltk sklearn pandas matplotlib seaborn\n",
        "data_dir = \"Machine-Learning-for-Security-Analysts\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_6QcEtvzX-E9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Common imports\n",
        "import re, os, math, string, json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import re\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "# Import Natural Language ToolKit library and download dictionaries\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "\n",
        "# Import Scikit-learn helper functions\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "\n",
        "# Import Scikit-learn models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "\n",
        "# Import Scikit-learn metric functions\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n### Libraries imported ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lhkR4nMLMmRT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test email from lecture slides\n",
        "test_email = \"\"\"\n",
        "Re: Re: East Asian fonts in Lenny. Thanks for your support.  Installing unifonts did it well for me. ;)\n",
        "Nima\n",
        "--\n",
        "To UNSUBSCRIBE, email to debian-user-REQUEST@lists.debian.org\n",
        "with a subject of \"unsubscribe\". Trouble? Contact listmaster@lists.debian.org\n",
        "\"\"\"\n",
        "print(test_email)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPtTuWPDhy_Y",
        "colab_type": "text"
      },
      "source": [
        "# Tokenization\n",
        "Continuing from where we left off with the slides, we'll start by creating our tokenizer."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qJKg9PaNMnA3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define tokenizer\n",
        "#   The purpose of a tokenizer is to separate the features from the raw data\n",
        "\n",
        "def tokenizer(text):\n",
        "  \"\"\"Separates feature words from the raw data\n",
        "  Keyword arguments:\n",
        "    text ---- The full email body\n",
        "    \n",
        "  :Returns -- The tokenized words; returned as a list\n",
        "  \"\"\"\n",
        "  \n",
        "  # Retrieve a list of punctuation characters, a list of stopwords, and a stemmer function\n",
        "  punctuations = list(string.punctuation)\n",
        "  stopwords = nltk.corpus.stopwords.words('english')\n",
        "  stemmer = nltk.stem.PorterStemmer()\n",
        "  \n",
        "  \n",
        "  # Set email body to lowercase, separate words and strip out punctuation\n",
        "  tokens = nltk.word_tokenize(text.lower())\n",
        "  tokens = [i.strip(''.join(punctuations)) \n",
        "            for i in tokens \n",
        "            if i not in punctuations]\n",
        "  \n",
        "  \n",
        "  # User Porter Stemmer on each token\n",
        "  tokens = [stemmer.stem(i)\n",
        "            for i in tokens]\n",
        "  return [w for w in tokens if w not in stopwords and w != \"\"]\n",
        "\n",
        "print(\"\\n### Tokenizer defined ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4BvaXgBOmxv",
        "colab_type": "text"
      },
      "source": [
        "## Task 1 - Tokenize an email\n",
        "1. Print the full email, **test_email**\n",
        "2. Print the results of **tokenizer(test_email)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dGV5z4DRPHG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's see how our tokenizer changes our email\n",
        "print(\"\\n- Test Email Body -\\n\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Tokenize test email\n",
        "print(\"\\n - Tokenized Output -\\n\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h_fentliZuA",
        "colab_type": "text"
      },
      "source": [
        "# Load Training Data\n",
        "With our tokenizer defined, let's take a look at our training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UT3pENI0iuMS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get counts of each class\n",
        "ham_count = len(os.listdir(data_dir+\"/ham\"))\n",
        "spam_count = len(os.listdir(data_dir+\"/spam\"))\n",
        "test_count = len(os.listdir(data_dir+\"/test\"))\n",
        "print(\"\\n### Class Counting Complete ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "js7XaCCEPJtZ",
        "colab_type": "text"
      },
      "source": [
        "## Task 2 - Load training data\n",
        "1. Create two *list()* arrays: **corpus** and **labels**\n",
        "2. Load the email bodies from from the **/ham** and **/spam** directories into the *corpus* array\n",
        "3. Load the labels for each email into the *labels* array\n",
        "\n",
        "\\* The lengths of *corpus* and *labels* should be identical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tD1gB8EUMn6Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the training data\n",
        "#   \"corpus\" is used to store all of the email bodies in a list\n",
        "#   \"labels\" is used to store all of the labes for those email bodies\n",
        "\n",
        "# Data and label arrays\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load all of the emails from the \"ham\" directory\n",
        "print(\"- Loading Ham -\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load all of the emails from the \"spam\" directory\n",
        "print(\"- Loading Spam -\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Loading Complete ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ckYcXPpq49Co",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print counts of each class\n",
        "count_classes = pd.value_counts(labels)\n",
        "print(\"Ham:\", count_classes['ham'])\n",
        "print(\"Spam:\", count_classes['spam'])\n",
        "\n",
        "# Graph counts of each class\n",
        "count_classes.plot(kind=\"bar\", fontsize=16)\n",
        "plt.title(\"Class count (training)\", fontsize=20)\n",
        "plt.xticks(rotation=\"horizontal\")\n",
        "plt.xlabel(\"Class\", fontsize=20)\n",
        "plt.ylabel(\"Class Count\", fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iAh5AQOwP72_",
        "colab_type": "text"
      },
      "source": [
        "## Task 2a (optional) - View training data\n",
        "1. Show the full text body of a random email in *corpus*\n",
        "2. Show the *tokenized* result of the same email"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mk4n6DvQMowG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's see how our corpus looks\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n- Email Body -\\n\")\n",
        "#(Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n- Tokenized Output -\\n\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4PmEWehhCDo",
        "colab_type": "text"
      },
      "source": [
        "# Vectorize the Data\n",
        "Now that the training data has been loaded, we'll train the vectorizers to turn our features into numbers.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oh5CNmcGQ_lk",
        "colab_type": "text"
      },
      "source": [
        "## Task 3 - Train the vectorizers\n",
        "1. Create the count vectorizer **cVec** using the **CountVectorizer** function\n",
        "2. Configure *cVec* to use the *tokenizer* function from earlier\n",
        "3. Perform **fit_transform** on *cVec* to train the vectorizer with the *corpus*\\\n",
        "a. Save the result as **count_X**\n",
        "\n",
        "\n",
        "4. Create the TF-IDF vectorizer **tVec** using the **TfidfVectorizer** function\n",
        "5. Configure *tVec* to use the *tokenizer* function from earlier\n",
        "6. Perform **fit_transform** on *tVec* to train the vectorizer with the *corpus*\\\n",
        "a. Save the result as **tfidf_X** "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7U5uXWGduem",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize the training inputs -- Takes about 90 seconds to complete\n",
        "#   There are two types of vectors: \n",
        "#     1. Count vectorizer\n",
        "#     2. Term Frequency-Inverse Document Frequency (TF-IDF)\n",
        "\n",
        "\n",
        "print(\"- Training Count Vectorizer -\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"- Training TF-IDF Vectorizer -\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Vectorizing Complete ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UujcLc_sUSdI",
        "colab_type": "text"
      },
      "source": [
        "## Task 3a (optional) - Count the test email tokens\n",
        "1. Print the count of each *token* from **test_email**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "az8e0qbLupMQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Manually perform term count on test_email\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLMzUWT5VQMk",
        "colab_type": "text"
      },
      "source": [
        "## Task 3b (optional) - View the test email vectorizers\n",
        "1. Create a new **CountVectorizer** and **TfidfVectorizer** for demonstration\n",
        "2. Train the new vectorizers on **test_email** using **fit_transform**\n",
        "3. Print the results of each *transform*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0Zk9sYugzDy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\\n- Count Vectorizer (test_email) -\\n\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print()\n",
        "print(\"=\"* 50)\n",
        "print()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"\\n- TFidf Vectorizer (test_email) -\\n\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeE--QtjjhIV",
        "colab_type": "text"
      },
      "source": [
        "# Load Testing Data\n",
        "With our vectorizers trained, we're going to use them to calculate the probabilities on our testing data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCkWLePj0W-b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# List first 10 files in \"test\" directory\n",
        "#   The labels for each email are in the file name\n",
        "os.listdir(data_dir + '/test')[:10]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8XRvFQiWYDB",
        "colab_type": "text"
      },
      "source": [
        "## Task 4 - Load the testing data\n",
        "1. Create two *list()* arrays: **test_corpus** and **test_labels**\n",
        "2. Load the email bodies from from the **/test** directory into the *test_corpus* array\n",
        "3. Load the labels for each email into the *test_labels* array\n",
        "\n",
        "\\* The lengths of *test_corpus* and *test_labels* should be identical"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sOZUSpyDzFq6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load the testing data\n",
        "#   \"test_corpus\" is used to store all of the email bodies in a list\n",
        "#   \"test_labels\" is used to store all of the labes for those email bodies\n",
        "\n",
        "# Data and label arrays\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "# Load all of the emails from the \"test\" directory\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Loading Complete ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3VqmIM81nGf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Print counts of each class\n",
        "count_test_classes = pd.value_counts(test_labels)\n",
        "print(\"Ham:\", count_test_classes['ham'])\n",
        "print(\"Spam:\", count_test_classes['spam'])\n",
        "\n",
        "# Graph counts of each class\n",
        "count_test_classes.plot(kind=\"bar\", fontsize=16)\n",
        "plt.title(\"Class Count (Testing)\", fontsize=20)\n",
        "plt.xticks(rotation=\"horizontal\")\n",
        "plt.xlabel(\"Class\", fontsize=20)\n",
        "plt.ylabel(\"Class Count\", fontsize=20)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L08z7zxuYNs3",
        "colab_type": "text"
      },
      "source": [
        "## Task 5 - Vectorize the testing data\n",
        "1. Use **cVec** to *transform* **test_corpus**\\\n",
        "a. Save the result as **test_count_X**\n",
        "\n",
        "2. Use **tVec** to *transform* **test_corpus**\\\n",
        "a. Save the result as **test_tfidf_X**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PoDt2Qzx2_qZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Vectorize the testing inputs -- Takes about 30 seconds to complete\n",
        "#   Use 'transform' instead of 'fit_transform' because we've already trained our vectorizer\n",
        "\n",
        "\n",
        "print(\"- Count Vectorizer -\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "print(\"- Tfidf Vectorizer -\")\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Vectorizing Complete ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UyiNCVFlloOL",
        "colab_type": "text"
      },
      "source": [
        "# Test and Evaluate the Models\n",
        "OK, we have our training data loaded and our testing data loaded. Now it's time to train and evaluate our models. \n",
        "\n",
        "But first, we're going to define a helper function to display our evaluation reports."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qD3LWdNbYvqZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define report generator\n",
        "# (No action Needed)\n",
        "\n",
        "def generate_report(cmatrix, score, creport):\n",
        "  \"\"\"Generates and displays graphical reports\n",
        "  Keyword arguments:\n",
        "    cmatrix - Confusion matrix generated by the model\n",
        "    score --- Score generated by the model\n",
        "    creport - Classification Report generated by the model\n",
        "    \n",
        "  :Returns -- N/A\n",
        "  \"\"\"\n",
        "  \n",
        "  # Generate confusion matrix heatmap\n",
        "  plt.figure(figsize=(5,5))\n",
        "  sns.heatmap(cmatrix, \n",
        "              annot=True, \n",
        "              fmt=\"d\", \n",
        "              linewidths=.5, \n",
        "              square = True, \n",
        "              cmap = 'Blues', \n",
        "              annot_kws={\"size\": 16}, \n",
        "              xticklabels=['ham', 'spam'], \n",
        "              yticklabels=['ham', 'spam'])\n",
        "\n",
        "  plt.xticks(rotation='horizontal', fontsize=16)\n",
        "  plt.yticks(rotation='horizontal', fontsize=16)\n",
        "  plt.xlabel('Actual Label', size=20);\n",
        "  plt.ylabel('Predicted Label', size=20);\n",
        "\n",
        "  title = 'Accuracy Score: {0:.4f}'.format(score)\n",
        "  plt.title(title, size = 20);\n",
        "\n",
        "  # Display classification report and confusion matrix\n",
        "  print(creport)\n",
        "  plt.show()\n",
        "  \n",
        "\n",
        "print(\"\\n### Report Generator Defined ###\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cndMVm3Qht9r",
        "colab_type": "text"
      },
      "source": [
        "## Task 6a - Train and evaluate the MNB-TFIDF model\n",
        "1. Create **mnb_tfidf** as a **MultinomialNB()** constructor\n",
        "2. Use **fit** to train *mnb_tfidf* on the training data (*tfidf_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_tfidf_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *mnb_tfidf* to calculate model accuracy; save the results as **score_mnb_tfidf**\\\n",
        "b. Use the **predict** function in *mnb_tfidf* to generate model predictions; save the results as **predictions_mnb_tfidf**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_mnb_tfidf**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_mnb_tfidf**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X62-VpY3MrUw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multinomial Naive Bayesian with TF-IDF\n",
        "\n",
        "# Train the model\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_mnb_tfidf, score_mnb_tfidf, creport_mnb_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7hyvnwxj5y-",
        "colab_type": "text"
      },
      "source": [
        "## Task 6b - Train and evaluate the MNB-Count model\n",
        "1. Create **mnb_count** as a **MultinomialNB()** constructor\n",
        "2. Use **fit** to train *mnb_count* on the training data (*count_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_count_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *mnb_count* to calculate model accuracy; save the results as **score_mnb_count**\\\n",
        "b. Use the **predict** function in *mnb_count* to generate model predictions; save the results as **predictions_mnb_count**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_mnb_count**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_mnb_count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rzTGHNfMsBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Multinomial Naive Bayesian with Count Vectorizer\n",
        "\n",
        "# Train the model\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_mnb_count, score_mnb_count, creport_mnb_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KmNRsISkTnc",
        "colab_type": "text"
      },
      "source": [
        "## Task 6c - Train and evaluate the LGS-TFIDF model\n",
        "1. Create **lgs_tfidf** as a **LogisticRegression()** constructor, using the **lbfgs** *solver*\n",
        "2. Use **fit** to train *lgs_tfidf* on the training data (*tfidf_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_tfidf_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *lgs_tfidf* to calculate model accuracy; save the results as **score_lgs_tfidf**\\\n",
        "b. Use the **predict** function in *lgs_tfidf* to generate model predictions; save the results as **predictions_lgs_tfidf**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_lgs_tfidf**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_lgs_tfidf**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yoc2efFxMsw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression with TF-IDF\n",
        "\n",
        "# Train the model\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_lgs_tfidf, score_lgs_tfidf, creport_lgs_tfidf)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgFq9cZ4kbzP",
        "colab_type": "text"
      },
      "source": [
        "## Task 6d - Train and evaluate the LGS-Count model\n",
        "1. Create **lgs_count** as a **LogisticRegression()** constructor, using the **lbfgs** *solver*\n",
        "2. Use **fit** to train *lgs_count* on the training data (*count_X*) and training labels (*labels*)\n",
        "3. Evaluate the model with the testing data (*test_count_X*) and testing labels (*test_labels*):\\\n",
        "a. Use the **score** function in *lgs_count* to calculate model accuracy; save the results as **score_lgs_count**\\\n",
        "b. Use the **predict** function in *lgs_count* to generate model predictions; save the results as **predictions_lgs_count**\\\n",
        "c. Generate the confusion matrix with **confusion_matrix**, using the predictons and labels; save the results as **cmatrix_lgs_count**\\\n",
        "d. Generate the classification report with **classification_report**, using the predictions and labels; save the results as **creport_lgs_count**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v5vj_np2Mtb4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression with Count Vectorizer\n",
        "\n",
        "# Train the model\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "# Test the mode (score, predictions, confusion matrix, classification report)\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n### Model Built ###\\n\")\n",
        "generate_report(cmatrix_lgs_count, score_lgs_count, creport_lgs_count)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NS37VaM0oEDJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Real world spam email (Retrieved 5/20/2019)\n",
        "\n",
        "test_spam_email = \"\"\"Your latest issue is available NOW! If you do not want issue notifications, click here to unsubscribe.\n",
        "logo\n",
        "Hi GEORGE,\n",
        "\n",
        "Your latest digital issue is available NOW!\n",
        "\n",
        "Enjoy all the latest from InStyle right on your phone, computer or tablet!\n",
        "\n",
        "View your library now.\n",
        "\n",
        "READ NOW\n",
        "Your digital issue is delivered by emagazines.com. To unsubscribe, go here. Do not reply to this email. For more information, review our Privacy Policy and customer care options visit Customer Support.\n",
        "\n",
        "Copyright © 2017 - 2019 eMagazines. All Rights Reserved. 230 W Huron St., Ste 500, Chicago, IL 60654\n",
        "\"\"\"\n",
        "print(test_spam_email)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0aUsH92lJzq",
        "colab_type": "text"
      },
      "source": [
        "## Task 7 - Make a prediction\n",
        "1. Use the *vectorizers* and *models* created to perform predictions on **test_email** and **test_spam_email**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7HPn0uJEZ7_H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Enter an email to be predicted\n",
        "# (Write code here)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# (Keep the following lines)\n",
        "print(\"\\n- Model Predictions -\\n\")\n",
        "print(\"MNB/TFIDF:\", test_email_mnb_tfidf_pred)\n",
        "print(\"MNB/Count:\", test_email_mnb_count_pred)\n",
        "\n",
        "print(\"LGS/TFIDF:\", test_email_lgs_tfidf_pred)\n",
        "print(\"LGS/Count:\", test_email_lgs_count_pred)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}